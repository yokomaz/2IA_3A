{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IRI7FTYegiU-"
      },
      "outputs": [],
      "source": [
        "# Import the packages we will use after\n",
        "import os\n",
        "import copy\n",
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as skl\n",
        "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from google.colab import drive\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (17, 5)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from the google drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "audio_data_path = \"/content/gdrive/MyDrive/fma_small.zip\"\n",
        "metadata_path = \"/content/gdrive/MyDrive/fma_metadata.zip\"\n",
        "\n",
        "with ZipFile(audio_data_path, 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()\n",
        "\n",
        "with ZipFile(metadata_path, 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()\n",
        "\n",
        "AUDIO_DIR = \"fma_small\"\n",
        "METADATA_DIR = \"fma_metadata\"\n",
        "\n",
        "def load(filepath):\n",
        "\n",
        "    filename = os.path.basename(filepath)\n",
        "\n",
        "    if 'features' in filename:\n",
        "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
        "\n",
        "    if 'echonest' in filename:\n",
        "        return pd.read_csv(filepath, index_col=0, header=[0, 1, 2])\n",
        "\n",
        "    if 'genres' in filename:\n",
        "        return pd.read_csv(filepath, index_col=0)\n",
        "\n",
        "    if 'tracks' in filename:\n",
        "        tracks = pd.read_csv(filepath, index_col=0, header=[0, 1])\n",
        "\n",
        "        COLUMNS = [('track', 'tags'), ('album', 'tags'), ('artist', 'tags'),\n",
        "                   ('track', 'genres'), ('track', 'genres_all')]\n",
        "        for column in COLUMNS:\n",
        "            tracks[column] = tracks[column].map(ast.literal_eval)\n",
        "\n",
        "        COLUMNS = [('track', 'date_created'), ('track', 'date_recorded'),\n",
        "                   ('album', 'date_created'), ('album', 'date_released'),\n",
        "                   ('artist', 'date_created'), ('artist', 'active_year_begin'),\n",
        "                   ('artist', 'active_year_end')]\n",
        "        for column in COLUMNS:\n",
        "            tracks[column] = pd.to_datetime(tracks[column])\n",
        "\n",
        "        SUBSETS = ('small', 'medium', 'large')\n",
        "        try:\n",
        "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
        "                    'category', categories=SUBSETS, ordered=True)\n",
        "        except (ValueError, TypeError):\n",
        "            # the categories and ordered arguments were removed in pandas 0.25\n",
        "            tracks['set', 'subset'] = tracks['set', 'subset'].astype(\n",
        "                     pd.CategoricalDtype(categories=SUBSETS, ordered=True))\n",
        "            \n",
        "        return tracks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD97uLjdhiG9",
        "outputId": "bdd38145-1d69-471f-8b1a-d5fffa67954d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tracks = load(METADATA_DIR + os.sep + 'tracks.csv')\n",
        "genres = load(METADATA_DIR + os.sep + 'genres.csv')\n",
        "features = load(METADATA_DIR + os.sep + 'features.csv')\n",
        "echonest = load(METADATA_DIR + os.sep + 'echonest.csv')\n",
        "\n",
        "np.testing.assert_array_equal(features.index, tracks.index)\n",
        "assert echonest.index.isin(tracks.index).all()"
      ],
      "metadata": {
        "id": "C46MkL6Cht05"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files with very short audio length\n",
        "bad_file_indexes = [98565, 98567, 98569, 99134, 108925, 133297] \n",
        "print(\"tracks.shape: \", tracks.shape)\n",
        "for idx in bad_file_indexes:\n",
        "  tracks = tracks.drop(idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLJ3I7oV2hXW",
        "outputId": "ee3f2de3-9f00-4b58-bac2-fc4d9bee8fb8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tracks.shape:  (106574, 52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get small subset from meta data\n",
        "small = tracks[tracks['set', 'subset'] <= 'small']\n",
        "audio_idx = small.index\n",
        "audio_labels = small[('track','genre_top')]"
      ],
      "metadata": {
        "id": "PHSgjJo7iGDn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_audio_path(audio_dir, track_id):\n",
        "    \"\"\"\n",
        "    Return the path to the mp3 given the directory where the audio is stored\n",
        "    and the track ID.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> get_audio_path(AUDIO_DIR, 2)\n",
        "    '../data/fma_small/000/000002.mp3'\n",
        "\n",
        "    \"\"\"\n",
        "    tid_str = '{:06d}'.format(track_id)\n",
        "    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')"
      ],
      "metadata": {
        "id": "wtJjhHu7jb2f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hash_table = {'Hip-Hop':0, 'Pop':1, 'Folk':2, 'Rock':3, 'Experimental':4, 'International':5, 'Electronic':6, 'Instrumental':7}\n",
        "def numeric_labels(audio_labels):\n",
        "  for i in hash_table:\n",
        "    audio_labels = audio_labels.replace(i, hash_table[i])\n",
        "  return audio_labels"
      ],
      "metadata": {
        "id": "_aug3eglyd_4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1\n",
        "# Define a class in order to load data from the FMA database\n",
        "# It will return a sample with a waveform and a label\n",
        "class CustomAudioDataset(Dataset):\n",
        "  def __init__(self, audio_dir, audio_labels):\n",
        "    self.audio_dir = audio_dir\n",
        "    self.audio_labels = audio_labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.audio_labels)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    track_id = self.audio_labels.iloc[idx][0]\n",
        "    label = self.audio_labels.iloc[idx][1]\n",
        "    \n",
        "    audio_path = get_audio_path(self.audio_dir, track_id)\n",
        "    x, sample_rate = librosa.load(audio_path, sr = None, mono = True) # -> use librosa instead\n",
        "    waveform = torch.tensor(x[None,:])\n",
        "    transform = transforms.Compose([torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=4096, hop_length=4096//4), transforms.Resize((128, 1200))]) \n",
        "    melspectro = transform(waveform)\n",
        "\n",
        "    return melspectro, label"
      ],
      "metadata": {
        "id": "zZ2_ALohg8g2"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2\n",
        "# Create the train and test subsets from the small dataset\n",
        "train_set = small[small['set','split'] == 'training'].reset_index()\n",
        "test_set = small[small['set','split'] == 'test'].reset_index()\n",
        "\n",
        "# Convert the genre name from string to digits\n",
        "train_set_label = numeric_labels(train_set.loc[:, [('track_id', ''), ('track','genre_top')]])\n",
        "test_set_label = numeric_labels(train_set.loc[:, [('track_id', ''), ('track','genre_top')]])"
      ],
      "metadata": {
        "id": "kmpKfuQj1mjm"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3\n",
        "# Create datasets (train and test)\n",
        "train_dataset = CustomAudioDataset(audio_dir=AUDIO_DIR, audio_labels=train_set_label)\n",
        "test_dataset = CustomAudioDataset(audio_dir=AUDIO_DIR, audio_labels=test_set_label)\n",
        "\n",
        "# Create dataloaders(train and test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\"\"\"\n",
        "# Iterate through the DataLoader\n",
        "train_features, train_label = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape:{train_features.size()}\")\n",
        "print(f\"Label batch shape:{train_label.size()}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hHdStFeZ5APA",
        "outputId": "9e7273b6-036e-454a-9bfc-47d269bec2c1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Iterate through the DataLoader\\ntrain_features, train_label = next(iter(train_dataloader))\\nprint(f\"Feature batch shape:{train_features.size()}\")\\nprint(f\"Label batch shape:{train_label.size()}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4\n",
        "# 4.1 Define the model\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
        "    self.linear1 = nn.Linear(in_features=8940, out_features=64, bias=True)\n",
        "    self.linear2 = nn.Linear(in_features=64, out_features=32, bias=True)\n",
        "    self.linear3 = nn.Linear(in_features=32, out_features=16, bias=True)\n",
        "    self.linear4 = nn.Linear(in_features=16, out_features=8, bias=True)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool( F.relu(x) )\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool( F.relu(x) )\n",
        "    x = self.flatten(x)\n",
        "    x = self.linear1(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.linear3(x)\n",
        "    x = self.linear4(x)\n",
        "    x = self.softmax(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "net = CNN()"
      ],
      "metadata": {
        "id": "3SyrElXNDjiA"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 Test the methode forward with a sample to make sure it works\n",
        "input,label = train_dataset.__getitem__(2)\n",
        "predict = net.forward(input)\n",
        "predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziLkzawJORjJ",
        "outputId": "992f6423-8c19-4eaa-aa67-7bda9179ecd0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9836e-17, 3.0829e-44,\n",
              "         1.0000e+00, 0.0000e+00]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5\n",
        "# Define the train_optim and train loop\n",
        "def train_optim(model, epochs, log_frequency, device):\n",
        "  # We assume that the test set plays the role of a validation set\n",
        "\n",
        "  model.to(device) # we make sure the model is on the proper device\n",
        "\n",
        "  # Multiclass classification setting, we use cross-entropy\n",
        "  # note that this implementation requires the logits as input \n",
        "  # logits: values prior softmax transformation \n",
        "  loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "  learning_rate = 1e-4\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "  for t in range(epochs):\n",
        "\n",
        "      model.train() # we specify that we are training the model\n",
        "\n",
        "      # At each epoch, the training set will be processed as a set of batches\n",
        "      for batch_id,  batch in enumerate(train_dataloader) : \n",
        "\n",
        "        images, labels  = batch\n",
        "\n",
        "        # we put the data on the same device\n",
        "        images, labels = images.to(device), labels.to(device)  \n",
        "        \n",
        "        y_pred = model(images) # forward pass output=logits\n",
        "\n",
        "        loss = loss_fn(y_pred, labels)\n",
        "\n",
        "        if batch_id % log_frequency == 0:\n",
        "            print(\"epoch: {:03d}, batch: {:03d}, loss: {:.3f} \".format(t+1, batch_id+1, loss.item()))\n",
        "\n",
        "        optimizer.zero_grad() # clear the gradient before backward\n",
        "        loss.backward()       # update the gradient\n",
        "\n",
        "        optimizer.step() # update the model parameters using the gradient\n",
        "\n",
        "      # Model evaluation after each step computing the accuracy\n",
        "      model.eval()\n",
        "      total = 0\n",
        "      correct = 0\n",
        "      for batch_id, batch in enumerate(test_dataloader):\n",
        "        images , labels = batch\n",
        "        images , labels = images.to(device), labels.to(device)\n",
        "        y_pred = model(images) # forward computes the logits\n",
        "        sf_y_pred = torch.nn.Softmax(dim=1)(y_pred) # softmax to obtain the probability distribution\n",
        "        _, predicted = torch.max(sf_y_pred , 1)     # decision rule, we select the max\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "      \n",
        "      print(\"[validation] accuracy: {:.3f}%\\n\".format(100 * correct / total))"
      ],
      "metadata": {
        "id": "0WjPyJqXOmb5"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6\n",
        "# Start the train and test loops\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "train_optim(net, epochs=3, log_frequency=60, device=device)"
      ],
      "metadata": {
        "id": "emgKSPRfBtJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7\n",
        "# Make some predictions on some examples\n",
        "input,label_real = train_dataset.__getitem__(3)\n",
        "predict = net.forward(input)\n",
        "print(\"The predict is: \")\n",
        "print(predict)\n",
        "print(\"Real label is: \" + str(label_real))"
      ],
      "metadata": {
        "id": "Wh7x1smFNfB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e3f5ad-dcb6-4ea2-c746-ca3c73c949bd"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predict is: \n",
            "tensor([[1.4656e-38, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
            "Real label is: 2\n"
          ]
        }
      ]
    }
  ]
}